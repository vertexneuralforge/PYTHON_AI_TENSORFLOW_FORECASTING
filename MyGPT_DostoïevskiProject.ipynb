{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0x_Wy4ORVCsZ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        " df = pd.read_fwf('dostov.txt', encoding='latin-1')\n",
        " print(df[:1000])\n",
        "# read it in to inspect it\n",
        "with open('dostov.txt', 'r', encoding='latin-1') as f:\n",
        "    text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0-G8hTCVFhp",
        "outputId": "330b4e91-6c70-4cc1-b1ce-cb4a4b19df8e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "length of dataset in characters:  1130530\n"
          ]
        }
      ],
      "source": [
        "print(\"length of dataset in characters: \", len(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWGScs4eVIsc",
        "outputId": "1543ae58-fbdb-4ae8-b7e7-7b6bb8dbed60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CRIME AND PUNISHMENT\n",
            "\n",
            "\n",
            "\n",
            "PART I\n",
            "\n",
            "\n",
            "\n",
            "CHAPTER I\n",
            "\n",
            "On an exceptionally hot evening early in July a young man came out of\n",
            "the garret in which he lodged in S. Place and walked slowly, as though\n",
            "in hesitation, towards K. bridge.\n",
            "\n",
            "He had successfully avoided meeting his landlady on the staircase. His\n",
            "garret was under the roof of a high, five-storied house and was more\n",
            "like a cupboard than a room. The landlady who provided him with garret,\n",
            "dinners, and attendance, lived on the floor below, and every time\n",
            "he went out he was obliged to pass her kitchen, the door of which\n",
            "invariably stood open. And each time he passed, the young man had a\n",
            "sick, frightened feeling, which made him scowl and feel ashamed. He was\n",
            "hopelessly in debt to his landlady, and was afraid of meeting her.\n",
            "\n",
            "This was not because he was cowardly and abject, quite the contrary; but\n",
            "for some time past he had been in an overstrained irritable condition,\n",
            "verging on hypochondria. He had become so completely absorbed in\n",
            "himself, and isola\n"
          ]
        }
      ],
      "source": [
        "# let's look at the first 1000 characters\n",
        "print(text[:1000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zXaLqB76VLGx",
        "outputId": "5096313a-1ccd-4bba-a31a-483272f82207"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " !()*,-.13456789:;?ABCDEFGHIJKLMNOPQRSTUVWXYZ[]_abcdefghijklmnopqrstuvwxyz¾ÒÓÔÕ\n",
            "91\n"
          ]
        }
      ],
      "source": [
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ElByr7CiKtcS",
        "outputId": "4bafe719-f121-4bc0-fa34-9c2037b4df0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWnkaCrRVOdL",
        "outputId": "129c5aad-2215-4d1d-f6d5-0e5b018cf664"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[56, 57, 57, 1, 68, 56, 53, 66, 53]\n",
            "hii there\n"
          ]
        }
      ],
      "source": [
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "print(encode(\"hii there\"))\n",
        "print(decode(encode(\"hii there\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Osj3wAZcVRAb",
        "outputId": "5ec4c672-a1df-45a2-f14d-9abb86750cc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1130530]) torch.int64\n",
            "tensor([22, 37, 28, 32, 24,  1, 20, 33, 23,  1, 35, 40, 33, 28, 38, 27, 32, 24,\n",
            "        33, 39,  0,  0,  0,  0, 35, 20, 37, 39,  1, 28,  0,  0,  0,  0, 22, 27,\n",
            "        20, 35, 39, 24, 37,  1, 28,  0,  0, 34, 62,  1, 49, 62,  1, 53, 72, 51,\n",
            "        53, 64, 68, 57, 63, 62, 49, 60, 60, 73,  1, 56, 63, 68,  1, 53, 70, 53,\n",
            "        62, 57, 62, 55,  1, 53, 49, 66, 60, 73,  1, 57, 62,  1, 29, 69, 60, 73,\n",
            "         1, 49,  1, 73, 63, 69, 62, 55,  1, 61, 49, 62,  1, 51, 49, 61, 53,  1,\n",
            "        63, 69, 68,  1, 63, 54,  0, 68, 56, 53,  1, 55, 49, 66, 66, 53, 68,  1,\n",
            "        57, 62,  1, 71, 56, 57, 51, 56,  1, 56, 53,  1, 60, 63, 52, 55, 53, 52,\n",
            "         1, 57, 62,  1, 38,  8,  1, 35, 60, 49, 51, 53,  1, 49, 62, 52,  1, 71,\n",
            "        49, 60, 59, 53, 52,  1, 67, 60, 63, 71, 60, 73,  6,  1, 49, 67,  1, 68,\n",
            "        56, 63, 69, 55, 56,  0, 57, 62,  1, 56, 53, 67, 57, 68, 49, 68, 57, 63,\n",
            "        62,  6,  1, 68, 63, 71, 49, 66, 52, 67,  1, 30,  8,  1, 50, 66, 57, 52,\n",
            "        55, 53,  8,  0,  0, 27, 53,  1, 56, 49, 52,  1, 67, 69, 51, 51, 53, 67,\n",
            "        67, 54, 69, 60, 60, 73,  1, 49, 70, 63, 57, 52, 53, 52,  1, 61, 53, 53,\n",
            "        68, 57, 62, 55,  1, 56, 57, 67,  1, 60, 49, 62, 52, 60, 49, 52, 73,  1,\n",
            "        63, 62,  1, 68, 56, 53,  1, 67, 68, 49, 57, 66, 51, 49, 67, 53,  8,  1,\n",
            "        27, 57, 67,  0, 55, 49, 66, 66, 53, 68,  1, 71, 49, 67,  1, 69, 62, 52,\n",
            "        53, 66,  1, 68, 56, 53,  1, 66, 63, 63, 54,  1, 63, 54,  1, 49,  1, 56,\n",
            "        57, 55, 56,  6,  1, 54, 57, 70, 53,  7, 67, 68, 63, 66, 57, 53, 52,  1,\n",
            "        56, 63, 69, 67, 53,  1, 49, 62, 52,  1, 71, 49, 67,  1, 61, 63, 66, 53,\n",
            "         0, 60, 57, 59, 53,  1, 49,  1, 51, 69, 64, 50, 63, 49, 66, 52,  1, 68,\n",
            "        56, 49, 62,  1, 49,  1, 66, 63, 63, 61,  8,  1, 39, 56, 53,  1, 60, 49,\n",
            "        62, 52, 60, 49, 52, 73,  1, 71, 56, 63,  1, 64, 66, 63, 70, 57, 52, 53,\n",
            "        52,  1, 56, 57, 61,  1, 71, 57, 68, 56,  1, 55, 49, 66, 66, 53, 68,  6,\n",
            "         0, 52, 57, 62, 62, 53, 66, 67,  6,  1, 49, 62, 52,  1, 49, 68, 68, 53,\n",
            "        62, 52, 49, 62, 51, 53,  6,  1, 60, 57, 70, 53, 52,  1, 63, 62,  1, 68,\n",
            "        56, 53,  1, 54, 60, 63, 63, 66,  1, 50, 53, 60, 63, 71,  6,  1, 49, 62,\n",
            "        52,  1, 53, 70, 53, 66, 73,  1, 68, 57, 61, 53,  0, 56, 53,  1, 71, 53,\n",
            "        62, 68,  1, 63, 69, 68,  1, 56, 53,  1, 71, 49, 67,  1, 63, 50, 60, 57,\n",
            "        55, 53, 52,  1, 68, 63,  1, 64, 49, 67, 67,  1, 56, 53, 66,  1, 59, 57,\n",
            "        68, 51, 56, 53, 62,  6,  1, 68, 56, 53,  1, 52, 63, 63, 66,  1, 63, 54,\n",
            "         1, 71, 56, 57, 51, 56,  0, 57, 62, 70, 49, 66, 57, 49, 50, 60, 73,  1,\n",
            "        67, 68, 63, 63, 52,  1, 63, 64, 53, 62,  8,  1, 20, 62, 52,  1, 53, 49,\n",
            "        51, 56,  1, 68, 57, 61, 53,  1, 56, 53,  1, 64, 49, 67, 67, 53, 52,  6,\n",
            "         1, 68, 56, 53,  1, 73, 63, 69, 62, 55,  1, 61, 49, 62,  1, 56, 49, 52,\n",
            "         1, 49,  0, 67, 57, 51, 59,  6,  1, 54, 66, 57, 55, 56, 68, 53, 62, 53,\n",
            "        52,  1, 54, 53, 53, 60, 57, 62, 55,  6,  1, 71, 56, 57, 51, 56,  1, 61,\n",
            "        49, 52, 53,  1, 56, 57, 61,  1, 67, 51, 63, 71, 60,  1, 49, 62, 52,  1,\n",
            "        54, 53, 53, 60,  1, 49, 67, 56, 49, 61, 53, 52,  8,  1, 27, 53,  1, 71,\n",
            "        49, 67,  0, 56, 63, 64, 53, 60, 53, 67, 67, 60, 73,  1, 57, 62,  1, 52,\n",
            "        53, 50, 68,  1, 68, 63,  1, 56, 57, 67,  1, 60, 49, 62, 52, 60, 49, 52,\n",
            "        73,  6,  1, 49, 62, 52,  1, 71, 49, 67,  1, 49, 54, 66, 49, 57, 52,  1,\n",
            "        63, 54,  1, 61, 53, 53, 68, 57, 62, 55,  1, 56, 53, 66,  8,  0,  0, 39,\n",
            "        56, 57, 67,  1, 71, 49, 67,  1, 62, 63, 68,  1, 50, 53, 51, 49, 69, 67,\n",
            "        53,  1, 56, 53,  1, 71, 49, 67,  1, 51, 63, 71, 49, 66, 52, 60, 73,  1,\n",
            "        49, 62, 52,  1, 49, 50, 58, 53, 51, 68,  6,  1, 65, 69, 57, 68, 53,  1,\n",
            "        68, 56, 53,  1, 51, 63, 62, 68, 66, 49, 66, 73, 18,  1, 50, 69, 68,  0,\n",
            "        54, 63, 66,  1, 67, 63, 61, 53,  1, 68, 57, 61, 53,  1, 64, 49, 67, 68,\n",
            "         1, 56, 53,  1, 56, 49, 52,  1, 50, 53, 53, 62,  1, 57, 62,  1, 49, 62,\n",
            "         1, 63, 70, 53, 66, 67, 68, 66, 49, 57, 62, 53, 52,  1, 57, 66, 66, 57,\n",
            "        68, 49, 50, 60, 53,  1, 51, 63, 62, 52, 57, 68, 57, 63, 62,  6,  0, 70,\n",
            "        53, 66, 55, 57, 62, 55,  1, 63, 62,  1, 56, 73, 64, 63, 51, 56, 63, 62,\n",
            "        52, 66, 57, 49,  8,  1, 27, 53,  1, 56, 49, 52,  1, 50, 53, 51, 63, 61,\n",
            "        53,  1, 67, 63,  1, 51, 63, 61, 64, 60, 53, 68, 53, 60, 73,  1, 49, 50,\n",
            "        67, 63, 66, 50, 53, 52,  1, 57, 62,  0, 56, 57, 61, 67, 53, 60, 54,  6,\n",
            "         1, 49, 62, 52,  1, 57, 67, 63, 60, 49])\n"
          ]
        }
      ],
      "source": [
        "# let's now encode the entire text dataset and store it into a torch.Tensor\n",
        "import torch # we use PyTorch: https://pytorch.org\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:1000]) # the 1000 characters we looked at earier will to the GPT look like this"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hkiC9Ok7VT9p"
      },
      "outputs": [],
      "source": [
        "# We split the data into train and validation sets\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6JR1uuCBVWzZ",
        "outputId": "7dba9c23-ca55-4ef0-b815-616f43a12251"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([22, 37, 28, 32, 24,  1, 20, 33, 23])"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "block_size = 8\n",
        "train_data[:block_size+1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LMVsjjEVYxI",
        "outputId": "dd4e0c9e-27ab-44a3-89fe-3ce951bc28ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "when input is tensor([22]) the target: 37\n",
            "when input is tensor([22, 37]) the target: 28\n",
            "when input is tensor([22, 37, 28]) the target: 32\n",
            "when input is tensor([22, 37, 28, 32]) the target: 24\n",
            "when input is tensor([22, 37, 28, 32, 24]) the target: 1\n",
            "when input is tensor([22, 37, 28, 32, 24,  1]) the target: 20\n",
            "when input is tensor([22, 37, 28, 32, 24,  1, 20]) the target: 33\n",
            "when input is tensor([22, 37, 28, 32, 24,  1, 20, 33]) the target: 23\n"
          ]
        }
      ],
      "source": [
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "for t in range(block_size):\n",
        "    context = x[:t+1]\n",
        "    target = y[t]\n",
        "    print(f\"when input is {context} the target: {target}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2ByN8O9VbPJ",
        "outputId": "5ad8cc6f-a0e9-49fd-a2d0-8572080fe73b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[ 1, 73, 63, 69,  1, 49, 60, 60],\n",
            "        [56, 49, 68,  1, 71, 66, 53, 68],\n",
            "        [53,  1, 71, 53, 66, 53,  0, 65],\n",
            "        [63, 60, 53, 62, 59, 49,  1, 54]])\n",
            "targets:\n",
            "torch.Size([4, 8])\n",
            "tensor([[73, 63, 69,  1, 49, 60, 60,  1],\n",
            "        [49, 68,  1, 71, 66, 53, 68, 51],\n",
            "        [ 1, 71, 53, 66, 53,  0, 65, 69],\n",
            "        [60, 53, 62, 59, 49,  1, 54, 63]])\n",
            "----\n",
            "when input is [1] the target: 73\n",
            "when input is [1, 73] the target: 63\n",
            "when input is [1, 73, 63] the target: 69\n",
            "when input is [1, 73, 63, 69] the target: 1\n",
            "when input is [1, 73, 63, 69, 1] the target: 49\n",
            "when input is [1, 73, 63, 69, 1, 49] the target: 60\n",
            "when input is [1, 73, 63, 69, 1, 49, 60] the target: 60\n",
            "when input is [1, 73, 63, 69, 1, 49, 60, 60] the target: 1\n",
            "when input is [56] the target: 49\n",
            "when input is [56, 49] the target: 68\n",
            "when input is [56, 49, 68] the target: 1\n",
            "when input is [56, 49, 68, 1] the target: 71\n",
            "when input is [56, 49, 68, 1, 71] the target: 66\n",
            "when input is [56, 49, 68, 1, 71, 66] the target: 53\n",
            "when input is [56, 49, 68, 1, 71, 66, 53] the target: 68\n",
            "when input is [56, 49, 68, 1, 71, 66, 53, 68] the target: 51\n",
            "when input is [53] the target: 1\n",
            "when input is [53, 1] the target: 71\n",
            "when input is [53, 1, 71] the target: 53\n",
            "when input is [53, 1, 71, 53] the target: 66\n",
            "when input is [53, 1, 71, 53, 66] the target: 53\n",
            "when input is [53, 1, 71, 53, 66, 53] the target: 0\n",
            "when input is [53, 1, 71, 53, 66, 53, 0] the target: 65\n",
            "when input is [53, 1, 71, 53, 66, 53, 0, 65] the target: 69\n",
            "when input is [63] the target: 60\n",
            "when input is [63, 60] the target: 53\n",
            "when input is [63, 60, 53] the target: 62\n",
            "when input is [63, 60, 53, 62] the target: 59\n",
            "when input is [63, 60, 53, 62, 59] the target: 49\n",
            "when input is [63, 60, 53, 62, 59, 49] the target: 1\n",
            "when input is [63, 60, 53, 62, 59, 49, 1] the target: 54\n",
            "when input is [63, 60, 53, 62, 59, 49, 1, 54] the target: 63\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(1337)\n",
        "batch_size = 4 # how many independent sequences will we process in parallel\n",
        "block_size = 8 # what is the maximum context length for predictions\n",
        "\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('----')\n",
        "\n",
        "for b in range(batch_size): # batch dimension\n",
        "    for t in range(block_size): # time dimension\n",
        "        context = xb[b, :t+1]\n",
        "        target = yb[b,t]\n",
        "        print(f\"when input is {context.tolist()} the target: {target}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PuEaVLQMVefd",
        "outputId": "3daeff50-decb-484f-d3ef-e4845fc41106"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 1, 73, 63, 69,  1, 49, 60, 60],\n",
            "        [56, 49, 68,  1, 71, 66, 53, 68],\n",
            "        [53,  1, 71, 53, 66, 53,  0, 65],\n",
            "        [63, 60, 53, 62, 59, 49,  1, 54]])\n"
          ]
        }
      ],
      "source": [
        "print(xb) # our input to the transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fASdvN1Vh1X",
        "outputId": "59b67d06-5f0c-47a2-b917-5f323e6cd6bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([32, 91])\n",
            "tensor(5.2529, grad_fn=<NllLossBackward0>)\n",
            "\n",
            "U]fDPxnT7Ei7NYpIpt4X]Õ:lOHBlw;ÓmQUoohfpm8;.ÕBRmVguHAÒoÔ*KVDw*Ppn\n",
            "LqJ5fpSnzBcapoÔ*X;hje\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "logits, loss = m(xb, yb)\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "\n",
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZnbW1U59VoQh"
      },
      "outputs": [],
      "source": [
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyKGXBnQVrcY",
        "outputId": "b13b334f-9ac1-4a1f-fc09-075e5e901007"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4.965620517730713\n"
          ]
        }
      ],
      "source": [
        "batch_size = 32\n",
        "for steps in range(100): # increase number of steps for good results...\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "laTmlA0xVt3b",
        "outputId": "f5245f11-e968-4e62-ad96-04d4b8c98e02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "KjÕ[Qfgf 8YNYFQRnoCBdQ?X-ZnL(Y*¾e!lÒN;WkRAÒ]MXOB7TG4 4JkÓSz¾mÓFMIV_4718OrmhIx71l;H4Q8uSVcEÕ:jyS?rEVgUÔtunU)ÔXXCDe_4mW3pL eu¾Inh8dvqEOÓIW.om1ZÔÓ?H4_9k1qÔQ[XoOO9KSz[QO*ZÔTN7ÓItÒVQ.Ó)Ôn4U9G::sJeLOP¾W! Y*JeDÕm8xnOaXLA!5dcB,3ÓY*),diskAsk5NcVK5dBÔDkvf??\n",
            "RqWJ7*_lLOÓ?:ysQ?DuRh,ÒÕc1O5uq]WxXZÔ!pu3¾9x1mdpSU!!-Õ6J*1NO8ZRGTwYSÔ5P¾T\n",
            "fS!TNPpjnH?ZVxK8J6_dpQM4CVqiyr14AÒe:!3Õ::Õ:tD.(*Ô7aZ6U1ÕiM5W3Õ6ATDDDGZ;8RoQRseaQ9Ji61qdYpgxn8arPBÕJ*Z!7QVNk6 f.1QSRi\n"
          ]
        }
      ],
      "source": [
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGWdpwZMVyC9",
        "outputId": "143f5f8c-c8f8-45b2-dc60-a5c09bb7b805"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "a=\n",
            "tensor([[1.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333]])\n",
            "--\n",
            "b=\n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "--\n",
            "c=\n",
            "tensor([[2.0000, 7.0000],\n",
            "        [4.0000, 5.5000],\n",
            "        [4.6667, 5.3333]])\n"
          ]
        }
      ],
      "source": [
        "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
        "torch.manual_seed(42)\n",
        "a = torch.tril(torch.ones(3, 3))\n",
        "a = a / torch.sum(a, 1, keepdim=True)\n",
        "b = torch.randint(0,10,(3,2)).float()\n",
        "c = a @ b\n",
        "print('a=')\n",
        "print(a)\n",
        "print('--')\n",
        "print('b=')\n",
        "print(b)\n",
        "print('--')\n",
        "print('c=')\n",
        "print(c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1GI9DLlV3ON",
        "outputId": "656b4679-3503-402d-ca36-e92b0598c966"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 2])"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# here we consider the following toy example:\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,2 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8z3RpbWZV6lT"
      },
      "outputs": [],
      "source": [
        "# We want to obtain x[b,t] = mean_{i<=t} x[b,i]\n",
        "xbow = torch.zeros((B,T,C))\n",
        "for b in range(B):\n",
        "    for t in range(T):\n",
        "        xprev = x[b,:t+1] # (t,C)\n",
        "        xbow[b,t] = torch.mean(xprev, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6DRB91SPV9V-",
        "outputId": "891734f1-15eb-4c13-81e6-346e862faf56"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# version 2: using matrix multiply for a weighted aggregation\n",
        "wei = torch.tril(torch.ones(T, T))\n",
        "wei = wei / wei.sum(1, keepdim=True)\n",
        "xbow2 = wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C)\n",
        "torch.allclose(xbow, xbow2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8PhH5KSKV_hp",
        "outputId": "d85bcda3-347f-4c9c-b248-05a24f50c043"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# version 3: use Softmax\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "xbow3 = wei @ x\n",
        "torch.allclose(xbow, xbow3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qjOltrGWBkN",
        "outputId": "79175c54-6c5f-4346-d221-a6f9c3020ad1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 16])"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# version 4: implementation of self-attention\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,32 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "# using a single Head to perform self-attention\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "k = key(x)   # (B, T, 16)\n",
        "q = query(x) # (B, T, 16)\n",
        "wei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
        "\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "#wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "v = value(x)\n",
        "out = wei @ v\n",
        "#out = wei @ x\n",
        "\n",
        "out.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZsQ5Vh1WFKm",
        "outputId": "746c96ee-43a5-48cd-c067-11010e9607f8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
              "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
              "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wei[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-Qt8LuCWIxs"
      },
      "outputs": [],
      "source": [
        "k = torch.randn(B,T,head_size)\n",
        "q = torch.randn(B,T,head_size)\n",
        "wei = q @ k.transpose(-2, -1) * head_size**-0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRyLz_KYWNIn",
        "outputId": "684538b4-fd37-49e4-ed5a-261fcb861072"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(1.0449)"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "k.var()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GvEIHUkqWPMd",
        "outputId": "551452bf-7881-44dc-ef04-1122660645f3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(1.0700)"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "q.var()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wg3wq0cVWQ9m",
        "outputId": "c1d8d4dc-cae5-46c1-f021-5d4f826798ec"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(1.0918)"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wei.var()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UIZZW_6vWS2Q",
        "outputId": "36999b44-8885-45ae-f047-8a87c1b4f13f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HyeWoNb8WVLz",
        "outputId": "716bc4de-29a6-473e-ef83-cc307af6ec8d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1) # gets too peaky, converges to one-hot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ru40Qq6XWYeG",
        "outputId": "bc166d74-0d5d-49ab-b285-f239f13627cc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([32, 100])"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class LayerNorm1d: # (used to be BatchNorm1d)\n",
        "\n",
        "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
        "    self.eps = eps\n",
        "    self.gamma = torch.ones(dim)\n",
        "    self.beta = torch.zeros(dim)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    # calculate the forward pass\n",
        "    xmean = x.mean(1, keepdim=True) # batch mean\n",
        "    xvar = x.var(1, keepdim=True) # batch variance\n",
        "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
        "    self.out = self.gamma * xhat + self.beta\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.gamma, self.beta]\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "module = LayerNorm1d(100)\n",
        "x = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\n",
        "x = module(x)\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XTCtID9WWbxZ",
        "outputId": "76269cf5-fbf0-4e25-e143-155f523b5d76"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor(0.1469), tensor(0.8803))"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x[:,0].mean(), x[:,0].std() # mean,std of one feature across all batch inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxQ79FOEWe51",
        "outputId": "8ac4a851-ebcf-45bf-a7b8-b47b8538fd6a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor(-9.5367e-09), tensor(1.0000))"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x[0,:].mean(), x[0,:].std() # mean,std of a single input from the batch, of its features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TUKmx1oHWg_U",
        "outputId": "6fd0fb21-cab0-4c1e-854d-949bb3737902"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.213083 M parameters\n",
            "step 0: train loss 4.6695, val loss 4.6676\n",
            "step 100: train loss 2.6710, val loss 2.6430\n",
            "step 200: train loss 2.5299, val loss 2.5177\n",
            "step 300: train loss 2.4415, val loss 2.4149\n",
            "step 400: train loss 2.3743, val loss 2.3497\n",
            "step 500: train loss 2.3043, val loss 2.2904\n",
            "step 600: train loss 2.2194, val loss 2.2051\n",
            "step 700: train loss 2.1606, val loss 2.1497\n",
            "step 800: train loss 2.1187, val loss 2.1189\n",
            "step 900: train loss 2.0524, val loss 2.0381\n",
            "step 1000: train loss 2.0239, val loss 2.0031\n",
            "step 1100: train loss 1.9894, val loss 1.9761\n",
            "step 1200: train loss 1.9570, val loss 1.9491\n",
            "step 1300: train loss 1.9269, val loss 1.9214\n",
            "step 1400: train loss 1.9105, val loss 1.8974\n",
            "step 1500: train loss 1.8802, val loss 1.8689\n",
            "step 1600: train loss 1.8662, val loss 1.8711\n",
            "step 1700: train loss 1.8495, val loss 1.8550\n",
            "step 1800: train loss 1.8281, val loss 1.8392\n",
            "step 1900: train loss 1.8098, val loss 1.8136\n",
            "step 2000: train loss 1.7857, val loss 1.7926\n",
            "step 2100: train loss 1.7826, val loss 1.7773\n",
            "step 2200: train loss 1.7734, val loss 1.7809\n",
            "step 2300: train loss 1.7548, val loss 1.7584\n",
            "step 2400: train loss 1.7354, val loss 1.7522\n",
            "step 2500: train loss 1.7387, val loss 1.7513\n",
            "step 2600: train loss 1.7223, val loss 1.7423\n",
            "step 2700: train loss 1.7145, val loss 1.7273\n",
            "step 2800: train loss 1.7073, val loss 1.7065\n",
            "step 2900: train loss 1.6993, val loss 1.7138\n",
            "step 3000: train loss 1.6835, val loss 1.6950\n",
            "step 3100: train loss 1.6660, val loss 1.6885\n",
            "step 3200: train loss 1.6643, val loss 1.6830\n",
            "step 3300: train loss 1.6611, val loss 1.6876\n",
            "step 3400: train loss 1.6597, val loss 1.6705\n",
            "step 3500: train loss 1.6554, val loss 1.6832\n",
            "step 3600: train loss 1.6349, val loss 1.6629\n",
            "step 3700: train loss 1.6377, val loss 1.6674\n",
            "step 3800: train loss 1.6189, val loss 1.6408\n",
            "step 3900: train loss 1.6301, val loss 1.6439\n",
            "step 4000: train loss 1.6245, val loss 1.6484\n",
            "step 4100: train loss 1.6314, val loss 1.6370\n",
            "step 4200: train loss 1.6175, val loss 1.6312\n",
            "step 4300: train loss 1.6079, val loss 1.6347\n",
            "step 4400: train loss 1.6076, val loss 1.6201\n",
            "step 4500: train loss 1.5962, val loss 1.6146\n",
            "step 4600: train loss 1.5918, val loss 1.6264\n",
            "step 4700: train loss 1.5875, val loss 1.6194\n",
            "step 4800: train loss 1.5818, val loss 1.6028\n",
            "step 4900: train loss 1.5688, val loss 1.5866\n",
            "step 5000: train loss 1.5783, val loss 1.5987\n",
            "step 5100: train loss 1.5671, val loss 1.5973\n",
            "step 5200: train loss 1.5707, val loss 1.5974\n",
            "step 5300: train loss 1.5602, val loss 1.5935\n",
            "step 5400: train loss 1.5569, val loss 1.5875\n",
            "step 5500: train loss 1.5645, val loss 1.5968\n",
            "step 5600: train loss 1.5457, val loss 1.5755\n",
            "step 5700: train loss 1.5478, val loss 1.5712\n",
            "step 5800: train loss 1.5496, val loss 1.5794\n",
            "step 5900: train loss 1.5423, val loss 1.5742\n",
            "step 6000: train loss 1.5435, val loss 1.5672\n",
            "step 6100: train loss 1.5283, val loss 1.5557\n",
            "step 6200: train loss 1.5359, val loss 1.5652\n",
            "step 6300: train loss 1.5295, val loss 1.5672\n",
            "step 6400: train loss 1.5283, val loss 1.5626\n",
            "step 6500: train loss 1.5207, val loss 1.5684\n",
            "step 6600: train loss 1.5264, val loss 1.5612\n",
            "step 6700: train loss 1.5180, val loss 1.5534\n",
            "step 6800: train loss 1.5238, val loss 1.5566\n",
            "step 6900: train loss 1.5226, val loss 1.5662\n",
            "step 7000: train loss 1.5122, val loss 1.5423\n",
            "step 7100: train loss 1.5213, val loss 1.5571\n",
            "step 7200: train loss 1.5097, val loss 1.5541\n",
            "step 7300: train loss 1.5121, val loss 1.5514\n",
            "step 7400: train loss 1.5008, val loss 1.5436\n",
            "step 7500: train loss 1.5161, val loss 1.5431\n",
            "step 7600: train loss 1.4992, val loss 1.5386\n",
            "step 7700: train loss 1.4991, val loss 1.5463\n",
            "step 7800: train loss 1.4983, val loss 1.5283\n",
            "step 7900: train loss 1.5030, val loss 1.5393\n",
            "step 8000: train loss 1.4911, val loss 1.5322\n",
            "step 8100: train loss 1.4904, val loss 1.5351\n",
            "step 8200: train loss 1.4875, val loss 1.5332\n",
            "step 8300: train loss 1.4909, val loss 1.5249\n",
            "step 8400: train loss 1.4847, val loss 1.5183\n",
            "step 8500: train loss 1.4853, val loss 1.5388\n",
            "step 8600: train loss 1.4917, val loss 1.5358\n",
            "step 8700: train loss 1.4818, val loss 1.5179\n",
            "step 8800: train loss 1.4867, val loss 1.5157\n",
            "step 8900: train loss 1.4816, val loss 1.5316\n",
            "step 9000: train loss 1.4769, val loss 1.5117\n",
            "step 9100: train loss 1.4701, val loss 1.5064\n",
            "step 9200: train loss 1.4811, val loss 1.5134\n",
            "step 9300: train loss 1.4632, val loss 1.5181\n",
            "step 9400: train loss 1.4731, val loss 1.5179\n",
            "step 9500: train loss 1.4647, val loss 1.5151\n",
            "step 9600: train loss 1.4675, val loss 1.5170\n",
            "step 9700: train loss 1.4700, val loss 1.5124\n",
            "step 9800: train loss 1.4679, val loss 1.5074\n",
            "step 9900: train loss 1.4590, val loss 1.4943\n",
            "step 9999: train loss 1.4613, val loss 1.5034\n",
            "\n",
            "\n",
            "ÒYes, I am not all a constupid by the\n",
            "onverwronds you hatural, within of unselful in\n",
            "Pastery!Ó\n",
            "\n",
            "He realices he your gyvephion; nated not upper imaginion yesterday.Ó Some ÒDidnÕt happen it?Ó Raskolnikov what there lost of cammotion could have been him\n",
            "might. Sonia...\n",
            "He canÕt you unteresting up and, she\n",
            "broke, Rodya, IÕll be uneasy\n",
            "am at that. It all\n",
            "for every pass swerity, After,\n",
            "you think the first her but him mied the does noyrow what.Ó\n",
            "\n",
            "Doc Dounia....Ó\n",
            "\n",
            "ThereÕs crushed Ôup my drink more\n",
            "mad, get we know,Ó\n",
            "\n",
            "ÒThe house in some feeling up, did, and get too,\n",
            "Imanity, take theyÕdÓ MrTain? FAcribing keed up\n",
            "one broke consciouse all lean it with the\n",
            "passar furthed.\n",
            "\n",
            "ÒIf was from going gave in storick to do you are at the face Petching he\n",
            "sage and hoped at the voice\n",
            "ring alky lost on me hand and the wonder\n",
            "of the old with her. It wast all seem at your other\n",
            "face..... She had sudk\n",
            "to neep full on stimpts. Her, sit..... LyDou they\n",
            "breath talking dark, dumined it from a longht, and moved and there will barker.\n",
            "And Dounia! I donÕt may doe. But sust me, too! To all as ark, are she was somewing of oneÕs prized\n",
            "on. Do.Ó Razumihin All , come AvaUW(A Pyotr Props marked him a long were crowd,\n",
            "ave they was talking. I donÕt wwant to more__,\n",
            "but was subject, quite\n",
            "two accused! For the qaverted question.\n",
            "\n",
            "How wan a crife. So there was for his bottom IÕve coming, commy at the way\n",
            "arript now made nothing his own and the prensent of. So that there was the latter from our Polest, imagining partually, we rewased with eyes of\n",
            "it from in a perforty made to be in his heard, he seeks otched with do why saying,\n",
            "too, and you fixing the controns. Fourwhand come and idead al* a; temper? Yes this is,Ó But he crusted,\n",
            "must why\n",
            "you love think where emticitions his reving to me! And what he\n",
            "does and you with men added at\n",
            "the stame; PotchÕs\n",
            "Are_, would seemed the table on on the\n",
            "dessched with a it, itÕs the douness I\n",
            "supped it too, came for you know?Ó\n",
            "\n",
            "KOU,Õ Pyotr-dom!Ó \n",
            "ÒYes? Twent it make to what and, backed of if you.Ó\n",
            "\n",
            "How you will absurd, she\n",
            "but now?Ó\n",
            "\n",
            "ÒAre may yes all whis trouby with ÔWhis droves, whis funed, he must make very ball.\n",
            "\n",
            "Sonia?Ó He went out told! But look you the are angry famicicith abcrusted alway. ÒUh, what mother, tell wonÕt that? To the live understandly if seem again, the\n",
            "like would that who this boway obsensed little you laught it almost\n",
            "him rather\n",
            "gremended to see is quite of freems of the artle! Oh, I could monther could bare\n",
            "this... And\n",
            "are abropped day I shall unterrey about it\n",
            "saircus drink, I told muth.... You\n",
            "are it this procken a came a mone. Porfiry his\n",
            "heart, and that does. I would the\n",
            "side?Ó\n",
            "\n",
            "ÒJo, you almost seemed him my vele deathing interest upon the\n",
            "and the moust brovousa Romanovna what tell, that he\n",
            "had not of mind to go the thness, closed thing weapted her way a mistronget.\n",
            "\n",
            "ÒThe looked somist know,Ó he went medimeds, Avdotya Romant\n",
            "Sonia, the navement to say_. ÒYes, they hand would he had sat!Ó One\n",
            "tell more would in slip of of the laughing like\n",
            "betty miscomment. There I am Ôpomet of being a grief four; mied, yar, which warmly any she donÕt believe what we into you want?Ó\n",
            "\n",
            "ÒWhy dear It help in a fortured looking at out oÕs, eveling of Pulcheria, why.\n",
            "\n",
            "ÒItÕs one spoke; yes midde suspicied to him bidde, long in the\n",
            "darkeoness and that that move two rish that,Ó she country remement now..... Suppering were what.... as you ever take him!Ó Lebeziatnikov to had deal the offushÕs only valited to minute her loe, keeping On, Dounia!) I requite a\n",
            "gloomy. Of course; Avdotya Ivanovna to be looked to coming out at\n",
            "once to everty Pooz, better girl even this wished. ÒTo this the Cman gesone, Raskolnikov go retile out to Vy, thou\n",
            "temptoreth, what what they fing?Ó\n",
            "\n",
            "ÒExcksist it neasual see seemed abovothes left ear now!Ó he sairs,\n",
            "friend to the old Raskolnikov at once? How I prove Ah6PTER\n",
            "PHAPTER VIIIvanoBut is meaning him remember inÕt\n",
            "be hanitures prefosent to hear seemed death\n",
            "nigh and sold of the\n",
            "pockst! Dmater word that it, and what he was not one to be upter.\n",
            "\n",
            "ÒWhy.....\n",
            "ItÕs...Ó\n",
            "\n",
            "ÒWhere, before I have not was\n",
            "good upon dresser and when\n",
            "attemptuous care, how feel.... He was justful part him at\n",
            "out! From thereÕs never under snything what after any you let me to him\n",
            "say, for the door.... Do you much a little himself, the sligh furional.\n",
            "\n",
            "ÒWell, donÕt know was petting would the copion\n",
            "still bewisgoned and leave with man his taking too, not\n",
            "to his pass is saying ald drops crue! And he\n",
            "dded into worlds... Finess we onlI.....Ó\n",
            "\n",
            "The had asked witha.Ó\n",
            "\n",
            "ÒYes.... Yes, you know I heard that I want should dread all\n",
            "the acver.\n",
            "\n",
            "ÒYes?. You saw how moment; there and that he was\n",
            "garve against from thous on ackstion cared alt his love, leow.. And the rags faits,\n",
            "about they are not drob-by the porter...\n",
            "Ah mount Òwere, there rith and alf saie everything\n",
            "that he was goes to him of tonÕt get laid,Ó\n",
            " almost he\n",
            "meet on all oÕcont, but it was nogs!Ó\n",
            "\n",
            "ÒAll I not! Leave awayon his will ready. And can on!...\n",
            "\n",
            "Pulcheria\n",
            "Alexandrovna,Ó Katerina 3PWere ah, looking whatÕs a rist....Ó He was strudth this be delirious. I shall art impossion the hand.\n",
            "ÒItÕs not know, thatÕs with all a second at the heart\n",
            "his young time.Ó\n",
            "\n",
            "ÒI get to stake in the tee. Anyway, the kirder? I had not hear, had heard, beginning supposed him their, his eldication;\n",
            "that the moneful had petsed to quite reach, cried always she was a bme\n",
            "see!Ó\n",
            "\n",
            "ÒOh, face the Northing and wouldnÕt given sumfister and fatutions! Why an and he donÕt no letter be\n",
            "downstairs of let go him). He was drunk with\n",
            "beneoting laughion, as the nyonew fred, the\n",
            "best in hreer within?Ó Freshed I partic\n",
            "it, as conscideded wuld he shakes... donÕt now an meants sort\n",
            "with me quezions, asking some window?Ó\n",
            "\n",
            "ÒWhis do you a keeps even I immed confainihins before\n",
            "Porfiry heart something; the tried you have\n",
            "jusiven his frunges that with corried Raskolnikov or aware on\n",
            "tur and for my are or hands\n",
            "and lofs, and which.... Bet that! No! Dom you\n",
            "are after to all of yourself sowing and bedid alk money with all me business, but a\n",
            "care and about she\n",
            "stood wasÓ that up upprobabably about of sometembned. ÒThe window you when I am batured.Ó\n",
            "\n",
            "He want?Ó\n",
            "\n",
            "ÒFort, you found he rushed of the would be nowehinÕ Never doubt,Ó crust behing befories,\n",
            "to deabs iltogisity in a puth. I wonÕt want a him and suddenly think_,\n",
            "even my think at he, and the workmen. But a huska, at times, and, as,\n",
            "and that you can almosy.... I trays this house lying when lour up\n",
            "me such.\n",
            "\n",
            "ÒDome, you heard and sixand.... But it all be pressed\n",
            "to it to Razumihin. There agonish were ones, looked donÕt mush\n",
            "and brother?\n",
            "It was nail the pillow, ÒAh, but you remember to beddest your the old can his ineasist. He upsteated him\n",
            "to unpression. On this faint it me\n",
            "on the\n",
            "powers at real time... And objectious\n",
            "blow... There was something asgide, who were this bstyly, are first and was a comn\n",
            "and you the escabit, took why are permove, put will hes best and caame of it.\n",
            "To the\n",
            "said.\n",
            "Atter her and rate.\n",
            "\n",
            "ÒOne began other eyes compation inscenessnÕt even debing, and all wome-that was very\n",
            "clear on the time\n",
            "han, Are\n",
            "IIvanovitch,Ó yes, whatÕs into your and not aware thut\n",
            "cave, though and the dea up\n",
            "brinnoment out it, tough Razumihin his bewildly to a speak and cannoced a frivent, but all\n",
            "gazve seemed you are as boart sank this simpless bowed.\n",
            "\n",
            "Why! And though you know. She only, then without of Razumihin are was entitured\n",
            "at heart. You know the were copness true to cide he tempted to hLidd a\n",
            "carefuls\n",
            "ugly seem now, donÕt walking condernt! And though on the same sound away to all preseness, before and in marked of his was ceamed, with\n",
            "the pot of clour. Souly ie what you\n",
            "us....Ó\n",
            "Oh, I am if the sturiously....Ó\n",
            "\n",
            "ÒYis decident him, before\n",
            "thatÕs all bott on thusing almost in the stration), he had to had been Polkinky was a fhour her most of open.\n",
            "\n",
            "ÒSide ye,Ó he assulked Gov.Ó\n",
            "\n",
            "ÒYou must his pale in whe thought, and the too... Some said matter--word you might will the edice word. Only when\n",
            "I capped and all least do yet in a dounk!\n",
            "To, donÕt shall canness this say. So that to like. Murthin.\n",
            "\n",
            "Sofya Petrovitch is,Ó know, botter, the grantly almoat.Ó\n",
            "\n",
            "ÒPulcheria Alexandrovna; thou noted\n",
            "to noticed know, and walked into may wwere but esporitious of\n",
            "up the room, people; people it that.Ó\n",
            "\n",
            "ÒAm and alrethat is he intered him. ÒI am brought was darking,Ó he answered went out a tell of a stifyl bridges, but the penefot?Ó\n",
            "\n",
            "Raskolnikov had before some was thatÕs in nowe,\n",
            "but I was vaniy not zussitov! Have like a later elements.\n",
            "\n",
            "ÒI  diz!Ó\n",
            "\n",
            "ÒI Somys,Ó Svya Petemyon Yes call, he cair be truth and\n",
            "prowed, was with blood in at cifty hing.... It was\n",
            "parceous throwe wrong a\n",
            "young.Õ\n",
            "\n",
            "ÒWhat it me. And up my espoud the windowher, in the may and the ring.\n",
            "\n",
            "ÒWhat on. Do you are at sudked the head\n",
            "afout, that all on conditial, towars\n",
            "from sounJ... Fundown and that I donÕt not?Ó\n",
            "\n",
            "They refushed afterward up you ask, just be sat all make, to desay.... It let all this education the man.\n",
            "\n",
            "ÒYou suppred that all him\n",
            "cast. ÒItÕs all thes all vinew starmer taken you all now he road\n",
            "_chest listed some rest better on.\n",
            "\n",
            "ÒMy lip were came of once his bell? Dunia.Ó\n",
            "\n",
            "ÒMarketed mowises his mindent, fell up at before it walked up like. Somewering, and\n",
            "you see, four she.Ó\n",
            "\n",
            "ÒOh? But we through, I didnÕt comfice she is\n",
            "from intered said to those cap full inquirious like. Afted the\n",
            "child taking. There I still\n",
            "be down could cursedly steps, I\n",
            "donÕt reflement him whick you like anyone, and\n",
            "Dounia know, into, was itÕs regarries, and with a polfosts like to then Petements, was went whis\n",
            "you retured me. What refused the clothes assoled in this\n",
            "time of quiting, do you can his turning But last\n",
            "in\n",
            "it a poor, and the god..... He impossibly bitter, too, yes, till you.Ó\n",
            "\n",
            "ÒSo you say, folew; I wault at the go wouldnÕt door.Ó\n",
            "\n",
            "ÒOut her believe, they had not like at a dam with lessious. Thort... Pulch thenes inst\n",
            "the box for, the strupt. ÒNow on the understand, IÕll and donÕt norhing you are the ipeadence into the safp enonihing regure and remove and the\n",
            "recondence, hour. ÒWell, some you come at that till every goes another for it?Ó Potherikov take he saw say part of him.... Poun chis went waved him again! I essumoved too you turn times in the pojolice widow,Ó bugged the\n",
            "course as went and you progs and though windows that he\n",
            "darked to the fact of life on my half,\n",
            "her. If that cough quite prowermont--lost her\n",
            "poscous. What much the thard what the fools of late any one plawate in Marke admit two with before, shudded cart of all the past in the changed you are\n",
            "his hand, and protsyation. It me,\n",
            "he wrets like and dreams to be plite\n",
            "about with. Willness Dounia some and first them with kept up embatrons, hou would not with his a potted to last expections and looked up about but down and the condern of held\n",
            "you are you will inuih... Far a studed and\n",
            "house. A hile wore\n",
            "bring interref I begged womanÕs met Fe! She us to be she made and proncension awledge\n",
            "let dexive to sold make to do say through is the letterwarday in the come, he goill canal....\n",
            "and I kneved him.Ó\n",
            "\n",
            "ÒHow crush it was evirend looking of gently love and sees to go thim long. If not picks in where wouldd he pief with be thought? Tell here\n",
            "anxious with Sofy and Zamething on here.\n",
            "And markly what was comphinium on to the was a prequent or that them. It is\n",
            "als you shake sometemened or solemently came vick no\n",
            "one. ItÕs with stood on For sister sit me object...\n",
            "In symbed with!Ó the yestes hangf,Ó they hoped up from so. Tas I wonÕt tick best people. Oth on oaking to absen yours eyed from nikov\n",
            "Kocked cases. No, it,Ó Raskolnikov closed it, the dead when you manÕt clurt and what, uddenta Petrovitch drawing with mary arm propoler excumberable many to late all him, Pavettrovna a from enlying one.Ó\n",
            "Preasy winer,Ó Potherima Pulcheria...Ó\n",
            "\n",
            "ÒI am no be doing beside of question, too, too, it had tonÕt everyone making the rivious hanÕ\n",
            "such was a batter live, we had sure sanking debject to, too, overcyed a lrinkly you self, theyÕd Raskolnikov had head endrough sheft for thrust, and abjustion significiising. Sosing that we slipped you seen\n",
            "of room anything way? She apped to cours, and\n",
            "crime.Ó\n",
            "\n",
            "ÒI was not -is shaking a gave all, I came seeper?Ó\n",
            "\n",
            "He wonther was only awarous tell it as, then a wold best will the profersting with all minute,Ó hoped het street. ÒBut you know as it wilky, to it talk! why was\n",
            "the put on the sofa\n",
            "yourself; that you\n",
            "are he couldnÕt tell a lifeve immagined, I\n",
            "will invery smit?Ó Avdotya\n",
            "Parka, by him word! They the whiaking up I have then almost, Òcrying last all, taking she once\n",
            "may. To my thigh\n",
            "yetsked where calt end on with\n",
            "windo lade, and you trld you like\n",
            "frowed. Douch you stand me!Ó He wanted it\n",
            "wonbank!Ó Koch alosed at (would reading back and desportions and there\n",
            "flatt! My\n",
            "lently door Dounia and just be paticulars.\n",
            "\n",
            "He axe here!Ó shouting all marked his cape that when\n",
            "heÕs sovenly docty was with some upon some disguition, a spared a hurs dayed with over news.\n",
            "\n",
            "They. I am which ir, you under very day after.. I love begar in\n",
            "here of Pullenka Petember, when it is must be in he doesnÕt\n",
            "dark face and sat he gave seeful such being\n",
            "when what all cwast her? Onper now,\n",
            "when are reasoness of themse: I was a coaping thing given to the mode to might, were\n",
            "a kindly Ivanovna wouldnÕt in doubt of se!Ó\n",
            " She was in siderinary room even bridged was he\n",
            "like him, glind; she ofice,Ó and he make two\n",
            "walking, and the hins, and down that\n",
            "that dismair. ÒGood alwashy crime.\n",
            "He fact a sight of him far, I Ôwould he pleasted by probab intermend tement in ideas.ÓHÕ Sofya PetrovnaÕ covert. It...Ó ÒSofya, DouniaÕs a faims; and get all _pully sreaming one barl oneÕs talking and drikes\n",
            "about lat\n",
            "humour in his help it sigking, more pold trifle. Dmitri Sofy\n",
            "Dounia, do, would\n",
            "blow sat, sFulk the too; we talk it we got to a moward_ Dounia at once bruts deaf?Ó\n",
            "\n",
            "ÒYou.Ó\n",
            "\n",
            "And trong\n",
            "Pyotr Petrovitch he wake you a rooming of on young his canÕt let\n",
            "fom seemed this book on the very\n",
            "as a money sock.\n",
            "A mome, and Ôpress nothing ones he\n",
            "she is gend it come midding and turned to bruth sometining, thatÕs a ear. At that Sonia, have\n",
            "linger looking, at micking vickitoonly. It me, triugh or worked at him. ÒIf itÕs\n",
            "nonse..veryone and not Raskolnikov had alvenly\n",
            "yourself\n",
            "his a sunkly.\n",
            "The saw Sofya I what not mared. ThatÕs talk you see of one\n",
            "as so are through. Somence flat, fast to\n",
            "on the sofa\n",
            "blows, consides. Helper upty whitDour candish even some now reals to death, lister case\n",
            "a upon that it long,Ó but was he came you, and I am almost trumphis with praption, the citur live,Ó the awful\n",
            "moment word, and this were slopped\n",
            "out alone shall especially and made\n",
            "of set, he knews. In ment why some all make into a flunging with he was Amalia Ivanovna And he big vick his polsible\n",
            "and forcend! Katerina Ivanov saw\n",
            "It provese give in the thirt, give it galt even to the plain gance, absuck up in last\n",
            "his absoln. Helpwered, and heighted and the\n",
            "same raisma was two ofter.Õ There art his like to go, but shalred that I\n",
            "danÕt delirious; though drove with thinking\n",
            "a sick we had not eveeding fright. (IÕll when trave persa Ivan taking over malial be and at ong of cap tle!Ó Razumihing for my money apperate and a wislike a very manight would steps the sudden\n",
            "only bress, which donÕt be is some and always he would not comfourion of the candled to the touch some tell Kix?Ó\n",
            "\n",
            "Òdepgains?... Am manÕt be it dear on the trongerous, do no need,\n",
            "a cradk awyou. Tell, but he put itÕs does into this possuous an everily to you, do,\n",
            "now to be can a sofa and with mark earn, and though in\n",
            "or better with that where was began bottly on the wold him fatherÕs.\n",
            "\n",
            "ÒIb,Ó\n",
            "\n",
            "ÒEh7orre thought he idead more with a clotwomant. Ah, I interry quite it. He conclusional Òlooking to the chims on the certary me, sees then the looked with a pulless. ÒIll me for service, itÕs he looked his\n",
            "shookings.Õ No; whick you\n",
            "me with a going\n",
            "exception cravoys with\n",
            "the uspicons..... Stry asked make inquively\n",
            "beside colious in Dounia, have aford, all his me for all the\n",
            "ward amous. Dot. The beat seet\n",
            "yes, interestly walked by his indeed\n",
            "by itcary a great) bottogen at awf. She was was not into out almong, and dreng with Good, but the could not first at\n",
            "all skill with sing then of mats moment satixed though\n",
            "almors. Bit.Ó\n",
            "\n",
            "All whis Pyotr Peterki\n",
            "liark Rodion,Ó Raskoled. ÒYou went that is say. The\n",
            "cried has sofa ashed it with is nots comphint. So\n",
            "kiss on the lodged of the dmeant up this pity blood to knows with I didnÕt\n",
            "know, intouted. That as a long for my notes awas to\n",
            "see of men, Sonia, when he wouldnÕt far sixter,Ó Paterchedly\n",
            "and actom anything watch only given others-\n",
            "fast is all tell the bmitt the artips all that I was.\n",
            "\n",
            "ÒSo It was neess it all the little fapt shakes as\n",
            "husbows and cosly the was the ranged\n",
            "cold and his and. I am till everything in the palement: I whis has been subjectiveicities as Randided, You know\n",
            "may for werench, gengling?Ó\n",
            "\n",
            "ÒHe seen almost him sick with her, and apon thirty book male abseoss)), moment he crect droman of upon there wants the face,\n",
            "irprestsiveny apped, voiced they let pitee into agream?Ó\n",
            "\n",
            "ÒShe was she talking a minuted him!Broke dead and smile that as seemed to him all, as though he\n",
            "had that! There was in a kirconder with\n",
            "enduted; and he sdong there live young some. Fut my head be death at the swor ay the portier) Am Polipt, the raepiing in\n",
            "the clupt I will know itÕs like and\n",
            "first of them?Ó\n",
            "\n",
            "ÒDonÕt speak into speak to his has beyond apperable; thoughÕ love must his had itÕs one beginned long of someting known exeduation! Do you are\n",
            "you knew at Pyotr Petching his blagge in he\n",
            "settick\n",
            "sad, with saw\n",
            "was allowat thereÕs gime and in revious may to asked by by key.\n",
            "\n",
            "She is distrighted and with cupinning-day, down for the\n",
            "warder. Pavemetov come stilking, but clame\n",
            "he was! A comport, of tavern\n",
            "at ongion other. He was life if forth; the I am leggimings to keeps with crime last comban! Do there wonÕt pretchmend with his whispered yours some taking for, plice\n",
            "and case of yoursely will\n",
            "not sing the simply! Mar he was seed up, he\n",
            "sure of he deven past eyes, yes, conscious with he with the\n",
            "long of tobbegar\n",
            "leadity as though smalight. He felt to propessive, I\n",
            "believe, that may a frehamed his compury. ÒRaskolnikov\n",
            "vrowing butto women one. How on the threes of\n",
            "owey. So Pulthered made that you are postured for with noticed without expression to-day! But what I am not\n",
            "it all musink my which his prosyed in silking eviling the-flashted to hims! Why befot even faight with Raskolnikov_, killed heiked... How of Pyotr Petrovitch what an he will just not refort to his sit down,\n",
            "and say, who had to pow?\n",
            "Till they he time anyt all, with the tudious not thing you all on\n",
            "so Pacht.Ó\n",
            "Raskolnikov, as myself its seemince that\n",
            "up on prossed to--comprecess--it? Never donÕt cateful me, itÕs in a his girlÕs\n",
            "to-day, a cruttmot believe, too, have\n",
            "yes... I longing.\n",
            "Where disgrimed and improssion brought\n",
            "the angry in his numbstanion days cried with with\n",
            "my first you may that it; frezumihin his again, and moment care an press... ÒDeling you wwent in manking to be do not providence and looked at his\n",
            "though him blood_, his read all time a steps ble that I can you uncertain conted with appered see he had seen and someountle drunken\n",
            "Karlering with like whe is might his tell call me abodyÕs seem?\n",
            "What happen too, dever child me ring. Raskolnikov\n",
            "almst....\n",
            "Why,Ó Peperium taking might to that or absettimennal you taks\n",
            "funed.Õ But as she doct her door looked stop at moniab the should teatidrovna.... He tell you you bar to end indeed\n",
            "the gate voice with you side me now, but sheÕs such the blood with defish, and\n",
            "arthing with.... What see them with op frotr Katch....\n",
            "And there harren but the hes\n",
            "fourt off, he, was some last inquiive another with mout his provens, but you that a bove all the bottook\n",
            "over began of his head with the familew serious some before waywas\n",
            "that work tearway! I was much always that heÕs ruifing about at the breate. ÒLotÕs\n",
            "what almost again, instanted the want of the\n",
            "wondow, why went you doing know ind\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 16 # how many independent sequences will we process in parallel?\n",
        "block_size = 32 # what is the maximum context length for predictions?\n",
        "max_iters = 10000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('dostov.txt', 'r', encoding='latin-1') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=20000)[0].tolist()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uB9tWi5jW08k"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
